{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP8IRbtC37UIyCklOx19jON",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhg86/LLM/blob/main/papier/Pythia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqQNIze3icYR",
        "outputId": "96e30bfc-d2f8-4535-e997-86837e7860d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import datasets\n",
        "\n",
        "from transformers import GPTNeoXForCausalLM, pipeline, AutoTokenizer\n"
      ],
      "metadata": {
        "id": "Jhm45OwKpuEL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FzMLQAhz9iB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pipeline = pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model = \"EleutherAI/pythia-160m-deduped\",\n",
        "#     revision=\"step143000\",\n",
        "#     cache_dir=\"./pythia-160m-deduped/step143000\"\n",
        "#     )\n",
        "\n",
        "# model = pipeline.model\n",
        "# tokenizer = pipeline.tokenizer\n",
        "# # model = GPTNeoXForCausalLM.from_pretrained(\n",
        "# #   \"EleutherAI/pythia-70m-deduped\",\n",
        "# #   revision=\"step143000\",\n",
        "# #   cache_dir=\"./pythia-70m-deduped/step143000\",\n",
        "# # )\n",
        "# # tokenizer = AutoTokenizer.from_pretrained(\n",
        "# #     \"EleutherAI/pythia-70m-deduped\",\n",
        "# #     revision=\"step143000\",\n",
        "# #     cache_dir=\"./pythia-70m-deduped/step143000\",\n",
        "# #     )\n",
        "\n",
        "# inputs = tokenizer(\"Paris is the capital of\", return_tensors=\"pt\")\n",
        "# tokens = model.generate(**inputs, max_length = 50)\n",
        "# tokenizer.decode(tokens[0])"
      ],
      "metadata": {
        "id": "RTXkRl0NilBD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Steer(nn.Module):\n",
        "  def __init__(self, lm_head, embed_dim, num_steers=2, rank=1000, init_var=1e-2, epsilon=1e-3):\n",
        "    super().__init__()\n",
        "    self.projector1 = nn.Linear(embed_dim, rank)\n",
        "    self.projector2 = nn.Linear(rank, embed_dim)\n",
        "    self.lm_head = lm_head\n",
        "    self.rank = rank\n",
        "    self.epsilon = epsilon\n",
        "    self.num_steers = num_steers\n",
        "    self.embed_dim = embed_dim\n",
        "    self.steer_values = torch.zeros(num_steers)\n",
        "    self.weight = self.weight()\n",
        "\n",
        "  def set_values(self, steer_values):\n",
        "    self.steer_values = steer_values\n",
        "\n",
        "  def forward(self, x):\n",
        "    delta = self.projector2(self.projector1(x) * self.steer_values.unsqueeze(1))\n",
        "    return self.lm_head(x + self.epsilon * delta)\n",
        "\n",
        "  def regularization_term(self):\n",
        "    return torch.norm(self.projector1.weight) + torch.norm(self.projector2.weight)\n",
        "\n",
        "  def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
        "    # Call the superclass's state_dict method to handle the destination argument\n",
        "    state_dict_ = super().state_dict(destination, prefix, keep_vars)\n",
        "\n",
        "    # Add your custom state to the dictionary\n",
        "    state_dict_[prefix + 'projector1'] = self.projector1.state_dict()\n",
        "    state_dict_[prefix + 'projector2'] = self.projector2.state_dict()\n",
        "    return state_dict_\n",
        "    return {\"projector1\": self.projector1, \"projector2\": self.projector2}\n",
        "\n",
        "  def load_state_dict(self, state_dict):\n",
        "    self.projector1 = state_dict[\"projector1\"]\n",
        "    self.projector2 = state_dict[\"projector2\"]\n",
        "\n",
        "  def weight(self):\n",
        "    return [self.projector1.weight, self.projector2.weight]"
      ],
      "metadata": {
        "id": "YlczFmagpkuv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, steer, tokenizer, n_steps=1000, lr=1e-2, training_steer=0, num_steers=2, max_length=256, regularization=1e-6):\n",
        "    data_iter = iter(dataloader)\n",
        "\n",
        "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    print(\"number of training steps:\", n_steps)\n",
        "    start_step = 0\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    pbar = tqdm(range(start_step, n_steps))\n",
        "\n",
        "    for step_i in pbar:\n",
        "        batch = next(data_iter, None)\n",
        "        if batch is None:\n",
        "            data_iter = iter(dataloader)\n",
        "            batch = next(data_iter, None)\n",
        "\n",
        "        cur_batch_size = len(batch[\"text\"])\n",
        "        batch_stance = torch.Tensor(batch[\"label\"]).to(device)\n",
        "        batch_stance = batch_stance.unsqueeze(1)\n",
        "        batch_text = batch[\"text\"]\n",
        "        tokenized = tokenizer(batch_text, padding=True, max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "        input_ids = torch.LongTensor(tokenized[\"input_ids\"]).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        attention_mask = torch.LongTensor(tokenized[\"attention_mask\"]).to(device)\n",
        "\n",
        "        steer.set_values(torch.Tensor(batch[\"label\"]).to(device))\n",
        "\n",
        "        position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        # print(\"inpu_ids : \", input_ids.shape)\n",
        "        # print(\"attention_mask : \", attention_mask.shape)\n",
        "        # print(\"position_ids : \", position_ids.shape)\n",
        "        # print(\"batch_stance : \", batch_stance)\n",
        "\n",
        "        output = model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, labels=input_ids)\n",
        "        loss = output.loss\n",
        "        regularization_term = steer.regularization_term()\n",
        "        (loss + regularization * regularization_term).backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    torch.save([\n",
        "        model.state_dict(),\n",
        "        max(n_steps, start_step)\n",
        "    ], \"train.pt\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QVo2nxBu2qQv"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = datasets.load_dataset(\"SetFit/sst5\")[\"train\"]\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j29jUMEc8tQk",
        "outputId": "56555dd8-ae05-4d4a-ec44-c9e0713957a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = \"EleutherAI/pythia-160m-deduped\",\n",
        "    revision=\"step143000\",\n",
        "    cache_dir=\"./pythia-160m-deduped/step143000\",\n",
        "    device= \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNpXgr419kNs",
        "outputId": "9942481b-87b1-4793-a9f5-b028e5bb6686"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = pipeline.model\n",
        "tokenizer = pipeline.tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "steer = Steer(model.embed_out, model.config.hidden_size)\n",
        "model.lm_head = steer\n",
        "\n",
        "# vocab_size = len(tokenizer)\n",
        "# model.resize_token_embeddings(vocab_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "yoGzDPPV-Fx8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(dataloader_train, model, steer, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTrCxpJkA-Mb",
        "outputId": "cd3137e4-4db6-448f-c052-1a1c26b8b4b2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training steps: 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [02:01<00:00,  8.25it/s]\n",
            "<ipython-input-39-6702b45d3061>:26: FutureWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
            "  state_dict_ = super().state_dict(destination, prefix, keep_vars)\n"
          ]
        }
      ]
    }
  ]
}